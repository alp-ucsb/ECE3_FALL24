{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483a5328",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 12 Least Squares Data Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbb83f",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src=\"figs/02_regression.png\" alt=\"Drawing\" width=600px/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ba7cd",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src=\"figs/12_regression.png\" alt=\"Drawing\" width=600px/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea4a854",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Unit 1: Vectors, Textbook Ch. 1-5\n",
    "\n",
    "#### Unit 2: Matrices, Textbook Ch. 6-11\n",
    "\n",
    "#### Unit 3: Least Squares, Textbook Ch. 12-14\n",
    "- 11 Least Squares\n",
    "- _**12 Least Squares Data Fitting**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87885fcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: House prices (Recall)\n",
    "\n",
    "$\\color{#047C91}{\\text{Example}}$: (Done in Lecture 02).\n",
    "\n",
    "- Outcome $\\hat{y}$: price of house in \\$1000.\n",
    "- Regressors $x$: (area, # bedrooms), area in 1000 sq.ft.\n",
    "\n",
    "We are given a regression model $\\hat y = w^Tx +b$ with weight vector and offset as:\n",
    "\n",
    "$$w^T = (148.73, −18.85), b = 54.40$$\n",
    "\n",
    "From this trained regression model, we can predict the price of a house with 2000 sq.ft and 3 bedrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a5ae7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Linear Regression Model (Recall)\n",
    "\n",
    "$\\color{#EF5645}{\\text{Definition}}$: A _linear regression model_ is the affine function:\n",
    "$$\\hat{y} = x^Tw + b$$\n",
    "where:\n",
    "- the $n$-vector $x$ is called a _feature_ or _input_ vector,\n",
    "- the $n$-vector $w$ is called _weight vector_,\n",
    "- the scalar $b$ is called _offset_ or _intercept_ or _bias_,\n",
    "- the scalar $\\hat y$ is called _prediction_ of an actual _outcome_ $y$.\n",
    "\n",
    "$\\color{#EF5645}{\\text{Question}}$: How do we find $w$ and $b$ in the first place?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987a2d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline: 12 Least Squares Data Fitting\n",
    "\n",
    "- **[Least Square Model Fitting](#sec)**\n",
    "- [Application to Regression](#sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3796889",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### True relationship: $f$\n",
    "\n",
    "\n",
    "$\\color{#EF5645}{\\text{Definition}}$: When we believe that a scalar $y$ and an $n$-vector $x$ are related by model:\n",
    "$$y ≈ f (x),$$\n",
    "we use the following vocabulary:\n",
    "- $x$ is called the feature or input vector,\n",
    "- $y$ is called the actual outcome or response variable,\n",
    "- $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ represents the \"true\" relationship between x and y.\n",
    "\n",
    "Generally, we do not know $f$, we just assume it exists. Our goal is to learn $f$, or a reasonable approximation of it, using data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c15a12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model: $\\hat{f}$\n",
    "\n",
    "$\\color{#EF5645}{\\text{Definition}}$: Choosing a set of \"basis functions\": $f_j: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, for $j=1...p$, we model a guess or approximation of $f$ as:\n",
    "$$\\hat y = \\hat{f}(x) = \\theta_1 f_1(x) + ... + \\theta_p f_p(x),$$\n",
    "where:\n",
    "- $\\theta_j$ are parameters that we learn from data: $x^{(1)}, ..., x^{(N)},..., y^{(1)},... y^{(N)}$,\n",
    "- $\\hat{y}^{(i)} = \\hat{f}(x^{(i)})$ is the prediction of $y^{(i)}$, for $i=1, ..., N$.\n",
    "\n",
    "$\\color{#047C91}{\\text{Exercise}}$: What are the basis functions in linear regression? What are the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20295c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction Error (Residual)\n",
    "\n",
    "$\\color{#EF5645}{\\text{Remark}}$: Our predictions are $\\hat y_i = \\hat f(x_i)$, for $i=1, ..., N$. If our model is good, then $\\hat{y}^{(i)} ≈ y^{(i)}$ for $i=1, ..., N$.\n",
    "\n",
    "\n",
    "$\\color{#EF5645}{\\text{Definition}}$: We define the _prediction error_, or _residual_ for each $i=1, ..., N$: \n",
    "$$r_i = y^{(i)} - \\hat{y}^{(i)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74860a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Least Square Data Fitting\n",
    "\n",
    "\n",
    "$\\color{#EF5645}{\\text{Definition}}$: The Least Square Data Fitting problem is the problem of choosing model's parameters $\\theta_1, ..., \\theta_n$ that minimize:\n",
    "$$\\big(\\frac{r_1^2 + ... + r_N^2}{N}\\big)^{1/2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd786b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\color{#6D7D33}{\\text{Proposition}}$: Define the $N \\times p$ matrix $A$ with elements $A_{ij} = f_j(x^{(i)})$, such that $\\hat y =A \\theta$, where $y = (y^{(1)}, . . . , y^{(N)})$ is vector of outcomes. \n",
    "\n",
    "The Least Square Data Fitting problem amounts to choose $\\theta$ that minimizes:\n",
    "$$||A\\theta - y||^2,$$\n",
    "which shows that it is effectively a Least Square Problem. \n",
    "\n",
    "$\\color{#6D7D33}{\\text{Proposition}}$: Consider the Least Square Data Fitting Problem. Assuming that the columns of $A$ are independent, the solution is:\n",
    "$$\\hat \\theta = (A^TA)^{-1}A^Ty.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c151ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline: 12 Least Squares Data Fitting\n",
    "\n",
    "- [Least Square Model Fitting](#sec)\n",
    "- **[Application to Regression](#sec)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e145eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression of House Prices\n",
    "\n",
    "Consider the model: $\\hat f (x) = \\theta_1 f_1(x) + \\theta_2 f_2(x)$ with $f_1(x) = 1$ and $f_2(x) = x$, i.e.:\n",
    "$$\\hat y = \\hat f (x) = \\theta_1 + \\theta_2 x,$$\n",
    "where:\n",
    "- $x$: house's area in 1000 sq feet,\n",
    "- $y$: house's price in k\\$.\n",
    "\n",
    "$\\color{#047C91}{\\text{Example}}$: Write down the LS problem associated to this learning. What are $A$, $y$? Explain how you can find $\\hat \\theta_1$ and $\\hat \\theta_2$ with Python. At home: compute $\\hat \\theta_1$ and $\\hat \\theta_2$ manually (hard)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8fce4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Unit 1: Vectors, Textbook Ch. 1-5\n",
    "\n",
    "#### Unit 2: Matrices, Textbook Ch. 6-11\n",
    "\n",
    "#### Unit 3: Least Squares, Textbook Ch. 12-14\n",
    "- 11 Least Squares\n",
    "- _**12 Least Squares Data Fitting**_\n",
    "\n",
    "Resources: Textbook Ch. 13-14."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
