{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483a5328",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 12 Least Squares Data Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbb83f",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src=\"figs/02_regression.png\" alt=\"Drawing\" width=600px/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ba7cd",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src=\"figs/12_regression.png\" alt=\"Drawing\" width=600px/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390cae36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Unit 1: Vectors, Textbook Ch. 1-5\n",
    "\n",
    "#### Unit 2: Matrices, Textbook Ch. 6-11\n",
    "\n",
    "#### Unit 3: Least Squares, Textbook Ch. 12-14\n",
    "- 11 Least Squares\n",
    "- _**12 Least Squares Data Fitting**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987a2d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Outline: 12 Least Squares Data Fitting\n",
    "\n",
    "- [Least Square Model Fitting](#sec)\n",
    "- [Application to Regression and Classification](#sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3796889",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### True relationship: $f$\n",
    "\n",
    "\n",
    "\n",
    "$\\color{#EF5645}{\\text{Definition}}$: When we believe that a scalar $y$ and an $n$-vector $x$ are related by model:\n",
    "$$y ≈ f (x),$$\n",
    "we use the following vocabulary:\n",
    "- $x$ is called the independent variable \n",
    "- $y$ is called the outcome or response variable\n",
    "- $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ represents the \"true\" relationship between x and y.\n",
    "\n",
    "Generally, we do not know $f$, we just assume it exists. Our goal is to learn $f$, or a reasonable approximation of it, using data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c15a12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model: $\\hat{f}$\n",
    "\n",
    "$\\color{#EF5645}{\\text{Definition}}$: Choosing a set of basis functions: $f_j: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, for $j=1...p$, we model a guess or approximation of $f$ as:\n",
    "$$\\hat{f}(x) = \\theta_1 f_1(x) + ... + \\theta_p f_p(x),$$\n",
    "where:\n",
    "- $\\theta_j$ are parameters that we learn from data: $x^{(1)}, ..., x^{(N)},..., y^{(1)},... y^{(N)}$,\n",
    "- $\\hat{y}^{(i)} = \\hat{f}(x^{(i)})$ is (the model’s) prediction of $y^{(i)}$, for $i=1, ..., N$.\n",
    "\n",
    "$\\color{#047C91}{\\text{Exercise}}$: What are the basis functions in linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20295c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction Error (Residual)\n",
    "\n",
    "$\\color{#EF5645}{\\text{Remark}}$: Our predictions are $\\hat y_i = \\hat f(x_i)$, for $i=1, ..., N$. If our model is good, then $\\hat{y}^{(i)} ≈ y^{(i)}$ for $i=1, ..., N$.\n",
    "\n",
    "\n",
    "$\\color{#EF5645}{\\text{Definition}}$: We define the _prediction error_, or _residual_ for each $i=1, ..., N$: \n",
    "$$r_i = y^{(i)} - \\hat{y}^{(i)}.$$\n",
    "\n",
    "$\\color{#047C91}{\\text{Exercise}}$: What is the average prediction error, averaged over the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74860a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Least Square Data Fitting\n",
    "\n",
    "\n",
    "$\\color{#EF5645}{\\text{Definition}}$: The Least Square Data Fitting problem is the problem of choosing model's parameters $\\theta_1, ..., \\theta_n$ that minimize the RMS prediction error on the dataset:\n",
    "$$\\big(\\frac{r_1^2 + ... + r_N^2}{N}\\big)^{1/2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd786b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\color{#6D7D33}{\\text{Proposition}}$: Define the $N \\times p$ matrix $A$ with elements $A_{ij} = f_j(x^{(i)})$, such that $\\hat y =A \\theta$, where $y = (y^{(1)}, . . . , y^{(N)})$ is vector of outcomes. The least square data fitting problem amounts to choose $\\theta$ that minimizes:\n",
    "$$||A\\theta - y||^2,$$\n",
    "which shows that it can be written as a Least Square Problem. Assuming that the columns of $A$ are independent, the solution is:\n",
    "$$\\hat \\theta = (A^TA)^{-1}A^Ty.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c151ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline: 12 Least Squares Data Fitting\n",
    "\n",
    "- [Least Square Model Fitting](#sec)\n",
    "- [Application to Regression and Classification](#sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926851a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification and Regression\n",
    "\n",
    "<center>  </center>\n",
    "<center><img src=\"figs/04_ai.png\" alt=\"default\" width=1500px/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e145eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression of House Prices\n",
    "\n",
    "- $x$: house's area in 1000 sq feet,\n",
    "- $y$: house's price in in k\\$\n",
    "(we do not consider the number of beds for simplicity).\n",
    "\n",
    "Consider the model: $\\hat f (x) = \\theta_1 f_1(x) + \\theta_2 f_2(x)$ with $f_1(x) = 1$ and $f_2(x) = x$, i.e.:\n",
    "$$\\hat f (x) = \\theta_1 + \\theta_2 x.$$\n",
    "\n",
    "$\\color{#047C91}{\\text{Example}}$: Write down the LS problem associated to this learning. What are $A$, $y$? Explain how you can find $\\hat \\theta_1$ and $\\hat \\theta_2$ with Python. At home: compute $\\hat \\theta_1$ and $\\hat \\theta_2$ manually (hard)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530ea6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From Regression to (Binary) Classification\n",
    "\n",
    "- Model $\\hat f$ outputs a number.\n",
    "- Binary classification wants a category: +1 or -1 only.\n",
    "\n",
    "$\\rightarrow$ use $sign(\\hat f)$ in place of $\\hat f$ to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0631f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification of MNIST\n",
    "\n",
    "- $x$ image of size $28 \\times 28$ from the MNIST dataset,\n",
    "- $y$: whether the image shows a $0$ digit or another digit.\n",
    "\n",
    "Consider the model: $\\hat f (x) = sign(\\theta_0 f_0(x) + \\theta_1 f_1(x) + ... + \\theta_{784} f_{784}(x))$ with $f_0(x) = 1$, and $f_p(x) = x_p$ for $p = 1, ..., 784$, i.e.:\n",
    "$$\\hat f (x) = sign(\\theta_0 + \\theta_1 x_1 + ... + \\theta_{784} x_{784}).$$\n",
    "\n",
    "$\\color{#047C91}{\\text{Example}}$: What are $x, y, A$? Explain how you can find the $\\hat \\theta$s with Python.\n",
    "\n",
    "\n",
    "<center><img src=\"figs/04_mnist.png\" alt=\"default\" width=250px/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d7985",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"figs/12_classification.png\" alt=\"default\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8fce4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Unit 1: Vectors, Textbook Ch. 1-5\n",
    "\n",
    "#### Unit 2: Matrices, Textbook Ch. 6-11\n",
    "\n",
    "#### Unit 3: Least Squares, Textbook Ch. 12-14\n",
    "- 11 Least Squares\n",
    "- _**12 Least Squares Data Fitting**_\n",
    "\n",
    "Resources: Textbook Ch. 13-14."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
